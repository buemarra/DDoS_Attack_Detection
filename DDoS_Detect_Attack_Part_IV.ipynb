{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc475e97-7cd3-45be-ba57-b7326c126aba",
   "metadata": {},
   "source": [
    "<h1> Sistema de Detección de Ciberataques DDoS por Análisis de Tráfico de Red </h1>\n",
    "<h2> Master en IA sobre el sector de la Energía y las infraestructuras </h2><br>\n",
    "\n",
    "<h4><b>Autor: </b> Ramiro Bueno Martínez</h4> \n",
    "<h4><b>Fecha: </b> 06/08/2025</h4> \n",
    "<h4><b>Version: </b> R200</h4> \n",
    "<h4><b>Descripcion: </b> PARTE IV: Implementación Distribuida basada en MAS</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deeb936-b430-4f85-b09c-f660771dd9b7",
   "metadata": {},
   "source": [
    "## Analisis de Tráfico de Red en Ataques DDoS\n",
    "El ataque de denegación de servicio (DDoS) es una amenaza a la seguridad de la red que tiene como objetivo agotar las redes de destino con tráfico malicioso. Aunque se han diseñado muchos métodos estadísticos para la detección de ataques DDoS, diseñar un detector en tiempo real con techos computacionales bajos sigue siendo una de las principales preocupaciones. Por otro lado, la evaluación de nuevos algoritmos y técnicas de detección se basa en gran medida en la existencia de conjuntos de datos bien diseñados.\n",
    "\n",
    "## Referencias estandares normativos\n",
    "<li><a src=\"https://www.iso.org/standard/74438.html\">ISO/IEC FDIS 23053 Framework for Artificial Intelligence (AI) Systems Using Machine Learning (ML)</a> Note: Under development ISO/IEC FDIS 23053</li>\n",
    "\n",
    "<li><a src=\"https://www.iso.org/standard/83002.html\">ISO/IEC CD 8183 Information technology — Artificial intelligence — Data life cycle framework</a> Note: Under development ISO/IEC CD 8183</li>\n",
    "\n",
    "<li><a src=\"https://www.iso.org/standard/78368.html\">ISO/IEC DIS 24668 Information technology — Artificial intelligence — Process management framework for big data analytics</a> Note: Under development ISO/IEC DIS 24668</li>\n",
    "\n",
    "<li><a src=\"https://www.iso.org/standard/81118.html\">ISO/IEC CD 5338 Information technology — Artificial intelligence — AI system life cycle processes</a> Note: Under developmentISO/IEC CD 5338</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631f0fd0-1feb-46cf-b7aa-8d04852b8f34",
   "metadata": {},
   "source": [
    "<h2> Objetivo Principal: </h2>\n",
    "\n",
    "El objetivo principal implementar un sistema distribuido de detección de posibles ataques DDoS a través de la Red mediante un sistema basado en agentes autonomos y multiagente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b419a2d-2f7f-4504-b76a-8f231f03bdf4",
   "metadata": {},
   "source": [
    "## Agente BDI\n",
    "\n",
    "Un Agente BDI (Beliefs, Desires, Intentions) es un modelo de agente racional que imita la toma de decisiones humana. Representa el estado mental de un agente de software y le permite razonar de manera autónoma en entornos dinámicos.\n",
    "- Creencias (Beliefs): El conocimiento o la información que el agente tiene sobre su entorno. Son hechos o percepciones que el agente considera verdaderos.\n",
    "- Deseos (Desires): Son los objetivos, tareas o estados que el agente quiere alcanzar. Representan el propósito o la motivación del agente.\n",
    "- Intenciones (Intentions): Son los planes de acción que el agente ha seleccionado para perseguir sus deseos. Son los compromisos del agente para ejecutar una secuencia de pasos.\n",
    "\n",
    "El modelo BDI proporciona un marco estructurado para que el agente reciba información (creencias), evalúe sus metas (deseos) y decida qué hacer (intenciones).\n",
    "\n",
    "### Agente BDI y Conciencia Situacional para la Detección de DDoS\n",
    "La Conciencia Situacional (SA) es la capacidad de percibir, comprender y proyectar el estado de un entorno. Para un Agente BDI, la Conciencia Situacional es el resultado de un ciclo continuo donde las creencias se actualizan a partir de datos del entorno, lo que a su vez impulsa los deseos y las intenciones del agente.\n",
    "\n",
    "En el contexto de la detección de ataques DDoS, un agente BDI dota de inteligencia al sistema de defensa de la siguiente manera:\n",
    "- Percepción y Creencias: El agente adquiere conciencia situacional del tráfico de red. En lugar de procesar los datos de red por sí mismo, utiliza un modelo de Machine Learning o Deep Learning preentrenado. Este modelo, entrenado con un conjunto de datos como el CICDDoS2019, actúa como el \"sentido\" del agente. Sus predicciones sobre la naturaleza del tráfico (normal, ataque) se convierten en las creencias del agente. Por ejemplo, si el modelo detecta un patrón de tráfico malicioso, el agente establece la creencia: \"Se está produciendo un ataque DoS de tipo UDP Flood.\"\n",
    "- Comprensión y Deseos: A partir de sus creencias, el agente comprende la situación y activa sus deseos. El deseo principal es mantener la disponibilidad y la estabilidad de la red. Esto se traduce en objetivos como \"mitigar el ataque\", \"minimizar la pérdida de paquetes\" y \"proteger los recursos críticos\".\n",
    "- Proyección e Intenciones: Con sus deseos claros y sus creencias actualizadas, el agente formula un plan de acción (intención). Este plan puede ser simple o complejo y se ejecuta para alcanzar sus deseos. Por ejemplo, el agente puede decidir:\n",
    "\n",
    "    - Bloquear las direcciones IP de origen sospechosas.\n",
    "    - Limitar la tasa de tráfico (rate-limiting) en un puerto específico.\n",
    "    - Generar una alerta y notificar a los administradores del sistema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8588047a-bb4c-413b-b4fe-d4ade971e2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "import threading\n",
    "import queue\n",
    "import joblib\n",
    "import pickle\n",
    "import logging\n",
    "from typing import Dict, List, Any, Optional\n",
    "from enum import Enum\n",
    "from dataclasses import dataclass, field\n",
    "import subprocess\n",
    "from io import StringIO\n",
    "import csv\n",
    "from sklearn.utils.validation import check_array\n",
    "from pathlib import Path  # Mejor manejo de rutas\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"X does not have valid feature names\")\n",
    "\n",
    "# Configuración de logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ThreatLevel(Enum):\n",
    "    \"\"\"Niveles de amenaza detectados\"\"\"\n",
    "    NONE = \"none\"\n",
    "    LOW = \"low\"\n",
    "    MEDIUM = \"medium\"\n",
    "    HIGH = \"high\"\n",
    "    CRITICAL = \"critical\"\n",
    "\n",
    "class AgentState(Enum):\n",
    "    \"\"\"Estados del agente BDI\"\"\"\n",
    "    IDLE = \"idle\"\n",
    "    CAPTURING = \"capturing\"\n",
    "    PROCESSING = \"processing\"\n",
    "    ANALYZING = \"analyzing\"\n",
    "    RESPONDING = \"responding\"\n",
    "\n",
    "@dataclass\n",
    "class Belief:\n",
    "    \"\"\"Representa una creencia del agente\"\"\"\n",
    "    key: str\n",
    "    value: Any\n",
    "    confidence: float\n",
    "    timestamp: datetime = field(default_factory=datetime.now)\n",
    "\n",
    "@dataclass\n",
    "class Desire:\n",
    "    \"\"\"Representa un deseo/objetivo del agente\"\"\"\n",
    "    goal: str\n",
    "    priority: int\n",
    "    active: bool = True\n",
    "\n",
    "@dataclass\n",
    "class Intention:\n",
    "    \"\"\"Representa una intención/plan del agente\"\"\"\n",
    "    action: str\n",
    "    parameters: Dict[str, Any]\n",
    "    expected_outcome: str\n",
    "    status: str = \"pending\"\n",
    "\n",
    "@dataclass\n",
    "class ThreatAlert:\n",
    "    \"\"\"Representa una alerta de amenaza\"\"\"\n",
    "    timestamp: datetime\n",
    "    threat_level: ThreatLevel\n",
    "    attack_type: Optional[str]\n",
    "    confidence: float\n",
    "    source_ip: str\n",
    "    destination_ip: str\n",
    "    details: Dict[str, Any]\n",
    "\n",
    "class NetworkCapture:\n",
    "    \"\"\"Módulo de captura de red basado en el código original\"\"\"\n",
    "    \n",
    "    def __init__(self, interface: str = \"wlp1s0\"):\n",
    "        self.interface = interface\n",
    "        self.output_dir_pcap = \"./pcap\"\n",
    "        self.output_dir_csv = \"./csv\"\n",
    "        self.output_dir_prod = \"./prod\"\n",
    "        self._setup_directories()\n",
    "    \n",
    "    def _setup_directories(self):\n",
    "        \"\"\"Crear directorios necesarios\"\"\"\n",
    "        for directory in [self.output_dir_pcap, self.output_dir_csv, self.output_dir_prod]:\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "    \n",
    "    def capture_traffic(self, duration: int) -> str:\n",
    "        \"\"\"Captura tráfico de red y retorna la ruta del archivo pcap\"\"\"\n",
    "        now = datetime.now()\n",
    "        timestamp = now.strftime(\"%d%m%Y-%H%M%S\")\n",
    "        #pcap_output_path = os.path.join(self.output_dir_pcap, f\"{timestamp}.pcap\")\n",
    "        # Usamos Path para garantizar compatibilidad multiplataforma\n",
    "        pcap_output_path = Path(self.output_dir_pcap) / f\"{timestamp}.pcap\"\n",
    "        # Convertimos a string y normalizamos los separadores\n",
    "        pcap_output_path = os.path.normpath(pcap_output_path)\n",
    "        \n",
    "        command = f\"tshark -i {self.interface} -a duration:{duration} -w {pcap_output_path} >> standard.out\"\n",
    "        logger.info(f\"Ejecutando captura: {command}\")\n",
    "        \n",
    "        result = os.system(command)\n",
    "        if result != 0:\n",
    "            raise Exception(f\"Error en la captura de tráfico: código {result}\")\n",
    "        \n",
    "        return pcap_output_path\n",
    "    \n",
    "    import os\n",
    "\n",
    "    def extract_flow_stats_with_tshark(self, pcap_path: str, output_csv: str) -> Optional[str]:\n",
    "        \"\"\"Extrae estadísticas de flujo similares a CICFlowMeter usando tshark + pandas.\n",
    "\n",
    "        Args:\n",
    "            pcap_path: Ruta al archivo .pcap.\n",
    "            output_csv: Ruta de salida para el CSV.\n",
    "    \n",
    "        Returns:\n",
    "            Ruta del CSV generado o None si hay error.\n",
    "        \"\"\"\n",
    "        # Lista de columnas esperadas (25 features)\n",
    "        expected_columns = [\n",
    "            'Source Port', 'Destination Port', 'Protocol', 'Flow Duration',\n",
    "            'Total Fwd Packets', 'Total Backward Packets', 'Total Length of Fwd Packets',\n",
    "            'Total Length of Bwd Packets', 'Fwd Packet Length Max', 'Fwd Packet Length Min',\n",
    "            'Fwd Packet Length Mean', 'Fwd Packet Length Std', 'Bwd Packet Length Max',\n",
    "            'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std',\n",
    "            'Flow IAT Min', 'Flow IAT Max', 'Flow IAT Mean', 'Flow IAT Std',\n",
    "            'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Flow Packets/s',\n",
    "            'Flow Bytes/s'\n",
    "        ]\n",
    "\n",
    "        # 1. Extraer datos crudos con tshark\n",
    "        command = [\n",
    "            \"tshark\",\n",
    "            \"-r\", pcap_path,\n",
    "            \"-T\", \"fields\",\n",
    "            \"-e\", \"frame.time_epoch\",\n",
    "            \"-e\", \"ip.src\",\n",
    "            \"-e\", \"ip.dst\",\n",
    "            \"-e\", \"tcp.srcport\", \"-e\", \"tcp.dstport\",\n",
    "            #\"-e\", \"udp.srcport\", \"-e\", \"udp.dport\",\n",
    "            \"-e\", \"udp.srcport\", \"-e\", \"udp.dstport\",  # <-- Línea corregida\n",
    "            \"-e\", \"ip.proto\",\n",
    "            \"-e\", \"frame.len\",\n",
    "            \"-E\", \"header=y\",\n",
    "            \"-E\", \"separator=,\",\n",
    "            \"-E\", \"quote=d\",\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            # Ejecutar tshark y cargar datos\n",
    "            result = subprocess.run(command, capture_output=True, text=True, check=True)\n",
    "            df = pd.read_csv(StringIO(result.stdout))\n",
    "        \n",
    "            # 2. Preprocesamiento de datos\n",
    "            df.columns = [\"timestamp\", \"src_ip\", \"dst_ip\", \n",
    "                         \"tcp_sport\", \"tcp_dport\", \"udp_sport\", \"udp_dport\",\n",
    "                         \"proto\", \"pkt_len\"]\n",
    "        \n",
    "            # Limpieza y conversión de tipos\n",
    "            df = df.fillna(0)\n",
    "            df[\"src_port\"] = df[\"tcp_sport\"].fillna(df[\"udp_sport\"]).astype(int)\n",
    "            df[\"dst_port\"] = df[\"tcp_dport\"].fillna(df[\"udp_dport\"]).astype(int)\n",
    "            df[\"proto\"] = df[\"proto\"].astype(int)\n",
    "\n",
    "            # 3. Agrupar por flujo\n",
    "            df_flows = df.groupby([\"src_ip\", \"dst_ip\", \"src_port\", \"dst_port\", \"proto\"])\n",
    "\n",
    "            # 4. Calcular estadísticas\n",
    "            flow_stats = []\n",
    "            for name, group in df_flows:\n",
    "                fwd = group[group[\"src_ip\"] == name[0]]\n",
    "                bwd = group[group[\"src_ip\"] == name[1]]\n",
    "            \n",
    "                flow_duration = max(group[\"timestamp\"].max() - group[\"timestamp\"].min(), 1e-6)\n",
    "            \n",
    "                # --- CORRECCIÓN: Definir 'flow_packets_rate' y 'flow_bytes_rate' aquí ---\n",
    "                total_packets = len(group)\n",
    "                total_bytes = group[\"pkt_len\"].sum()\n",
    "    \n",
    "                flow_packets_rate = total_packets / flow_duration\n",
    "                flow_bytes_rate = total_bytes / flow_duration\n",
    "                # Calcular las diferencias de tiempo entre paquetes consecutivos en todo el flujo\n",
    "                # La primera diferencia es NaN, por lo que la eliminamos (.iloc[1:])\n",
    "                diffs = group[\"timestamp\"].sort_values().diff().iloc[1:]\n",
    "    \n",
    "                # Calcular las diferencias de tiempo entre paquetes consecutivos en el flujo de avance (fwd)\n",
    "                fwd_diffs = fwd[\"timestamp\"].sort_values().diff().iloc[1:]\n",
    "                stats = {\n",
    "                    \"Source Port\": int(name[2]),\n",
    "                    \"Destination Port\": int(name[3]),\n",
    "                    \"Protocol\": int(name[4]),\n",
    "                    \"Flow Duration\": float(flow_duration),\n",
    "                    \"Total Fwd Packets\": int(len(fwd)),\n",
    "                    \"Total Backward Packets\": int(len(bwd)),\n",
    "                    \"Total Length of Fwd Packets\": int(fwd[\"pkt_len\"].sum()),\n",
    "                    \"Total Length of Bwd Packets\": int(bwd[\"pkt_len\"].sum() if len(bwd) > 0 else 0),\n",
    "                    \"Fwd Packet Length Max\": int(fwd[\"pkt_len\"].max()) if len(fwd) > 0 else 0,\n",
    "                    \"Fwd Packet Length Min\": int(fwd[\"pkt_len\"].min()) if len(fwd) > 0 else 0,\n",
    "                    \"Fwd Packet Length Mean\": float(fwd[\"pkt_len\"].mean()) if len(fwd) > 0 else 0.0,\n",
    "                    \"Fwd Packet Length Std\": float(fwd[\"pkt_len\"].std()) if len(fwd) > 1 else 0.0,\n",
    "                    \"Bwd Packet Length Max\": int(bwd[\"pkt_len\"].max()) if len(bwd) > 0 else 0,\n",
    "                    \"Bwd Packet Length Min\": int(bwd[\"pkt_len\"].min()) if len(bwd) > 0 else 0,\n",
    "                    \"Bwd Packet Length Mean\": float(bwd[\"pkt_len\"].mean()) if len(bwd) > 0 else 0.0,\n",
    "                    \"Bwd Packet Length Std\": float(bwd[\"pkt_len\"].std()) if len(bwd) > 1 else 0.0,\n",
    "                    \"Flow IAT Min\": float(diffs.min()) if len(diffs) > 0 else 0.0,\n",
    "                    \"Flow IAT Max\": float(diffs.max()) if len(diffs) > 0 else 0.0,\n",
    "                    \"Flow IAT Mean\": float(diffs.mean()) if len(diffs) > 0 else 0.0,\n",
    "                    \"Flow IAT Std\": float(diffs.std()) if len(diffs) > 1 else 0.0,\n",
    "                    \"Fwd IAT Total\": float(fwd_diffs.sum()) if len(fwd_diffs) > 0 else 0.0,\n",
    "                    \"Fwd IAT Mean\": float(fwd_diffs.mean()) if len(fwd_diffs) > 0 else 0.0,\n",
    "                    \"Fwd IAT Std\": float(fwd_diffs.std()) if len(fwd_diffs) > 1 else 0.0,\n",
    "                    \"Flow Packets/s\": float(flow_packets_rate),\n",
    "                    \"Flow Bytes/s\": float(flow_bytes_rate)\n",
    "                }\n",
    "                flow_stats.append(stats)\n",
    "            # 5. Crear DataFrame y asegurar las 25 columnas\n",
    "            df_result = pd.DataFrame(flow_stats).fillna(0)\n",
    "        \n",
    "            # Verificar columnas antes de guardar\n",
    "            missing_cols = set(expected_columns) - set(df_result.columns)\n",
    "            if missing_cols:\n",
    "                raise ValueError(f\"Columnas faltantes: {missing_cols}\")\n",
    "\n",
    "            # Guardar CSV con formato controlado\n",
    "            df_result[expected_columns].to_csv(\n",
    "                output_csv,\n",
    "                index=False,\n",
    "                float_format='%.6f',\n",
    "                quoting=csv.QUOTE_NONNUMERIC\n",
    "            )\n",
    "            return output_csv\n",
    "\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error en tshark (¿está instalado?): {e.stderr}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error inesperado: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    def convert_pcap_to_csv(self, pcap_path: str, output_dir: str) -> Optional[str]:\n",
    "        \"\"\"Convierte un archivo PCAP a CSV usando tshark (Wireshark).\n",
    "\n",
    "        Args:\n",
    "            pcap_path: Ruta completa al archivo .pcap.\n",
    "            output_dir: Directorio donde se guardará el CSV.\n",
    "        \n",
    "        Returns:\n",
    "            Ruta del archivo CSV generado o None si hay error.\n",
    "        \n",
    "        Raises:\n",
    "            Exception: Si tshark no está instalado o falla la conversión.\n",
    "        \"\"\"\n",
    "        # Validar entrada\n",
    "        if not os.path.isfile(pcap_path):\n",
    "            raise FileNotFoundError(f\"Archivo PCAP no encontrado: {pcap_path}\")\n",
    "    \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "        # Nombre del CSV (mismo nombre que el PCAP pero con extensión .csv)\n",
    "        timestamp = os.path.splitext(os.path.basename(pcap_path))[0]\n",
    "        csv_output_path = os.path.join(output_dir, f\"{timestamp}_tshark.csv\")\n",
    "    \n",
    "        # Campos a extraer (personaliza según necesidades)\n",
    "        fields = [\n",
    "            \"frame.time\", \"ip.src\", \"ip.dst\", \"ip.proto\",\n",
    "            \"tcp.srcport\", \"tcp.dstport\", \"udp.srcport\", \"udp.dstport\",\n",
    "            \"frame.len\", \"tcp.flags\", \"udp.length\", \"ip.ttl\"\n",
    "        ]\n",
    "    \n",
    "        # Comando tshark\n",
    "        command = [\n",
    "            \"tshark\",\n",
    "            \"-r\", pcap_path,  # Archivo de entrada\n",
    "            \"-T\", \"fields\",\n",
    "            *[f\"-e {field}\" for field in fields],\n",
    "            \"-E\", \"header=y\",  # Incluir cabeceras\n",
    "            \"-E\", \"separator=,\",  # CSV\n",
    "            \"-E\", \"quote=d\",  # Comillas dobles\n",
    "        ]\n",
    "    \n",
    "        # Ejecutar y redirigir salida al archivo CSV\n",
    "        try:\n",
    "            with open(csv_output_path, \"w\") as csv_file:\n",
    "                subprocess.run(\n",
    "                    command,\n",
    "                    stdout=csv_file,\n",
    "                    stderr=subprocess.PIPE,\n",
    "                    check=True,\n",
    "                    text=True\n",
    "                )\n",
    "            return csv_output_path\n",
    "        \n",
    "        except subprocess.CalledProcessError as e:\n",
    "            error_msg = f\"Error en tshark (código {e.returncode}): {e.stderr}\"\n",
    "            raise Exception(error_msg) from e\n",
    "        except FileNotFoundError:\n",
    "            raise Exception(\"tshark no está instalado. Instala Wireshark primero.\")\n",
    "    \n",
    "    def preprocess_for_classification(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Elimina columnas no necesarias y asegura 25 features\"\"\"\n",
    "        # Eliminar columna 'Unnamed: 0' si existe\n",
    "        if 'Unnamed: 0' in df.columns:\n",
    "            df = df.drop(columns=['Unnamed: 0'])\n",
    "    \n",
    "        # Verificar que tengamos exactamente 25 columnas\n",
    "        expected_columns = [\n",
    "            'Source Port', 'Destination Port', 'Protocol', 'Flow Duration',\n",
    "            'Total Fwd Packets', 'Total Backward Packets', 'Total Length of Fwd Packets',\n",
    "            'Total Length of Bwd Packets', 'Fwd Packet Length Max', 'Fwd Packet Length Min',\n",
    "            'Fwd Packet Length Mean', 'Fwd Packet Length Std', 'Bwd Packet Length Max',\n",
    "            'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std',\n",
    "            'Flow IAT Min', 'Flow IAT Max', 'Flow IAT Mean', 'Flow IAT Std',\n",
    "            'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Flow Packets/s',\n",
    "            'Flow Bytes/s'\n",
    "        ]\n",
    "    \n",
    "        # Reordenar columnas si es necesario\n",
    "        return df[expected_columns]\n",
    "\n",
    "    def validate_columns(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        expected_columns = [\n",
    "            'Source Port', 'Destination Port', 'Protocol', 'Flow Duration',\n",
    "            'Total Fwd Packets', 'Total Backward Packets', 'Total Length of Fwd Packets',\n",
    "            'Total Length of Bwd Packets', 'Fwd Packet Length Max', 'Fwd Packet Length Min',\n",
    "            'Fwd Packet Length Mean', 'Fwd Packet Length Std', 'Bwd Packet Length Max',\n",
    "            'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std',\n",
    "            'Flow IAT Min', 'Flow IAT Max', 'Flow IAT Mean', 'Flow IAT Std',\n",
    "            'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Flow Packets/s',\n",
    "            'Flow Bytes/s'\n",
    "        ]\n",
    "    \n",
    "        # Verificar si todas las columnas esperadas existen\n",
    "        missing = set(expected_columns) - set(df.columns)\n",
    "        if missing:\n",
    "            raise ValueError(f\"Columnas faltantes en el DataFrame: {missing}\")\n",
    "    \n",
    "        return df[expected_columns]\n",
    "\n",
    "    def reduce_dataset(self, csv_path: str) -> str:\n",
    "        \"\"\"Reduce el dataset a las columnas necesarias\"\"\"\n",
    "        try:\n",
    "            pcap_data = pd.read_csv(csv_path)\n",
    "            pcap_data = self.validate_columns(pcap_data)\n",
    "            logger.info(f\"Dataset cargado: {csv_path}, shape: {pcap_data.shape}\")\n",
    "            \n",
    "            # Campos seleccionados del código original\n",
    "            selected_fields = [\n",
    "                'Source Port', 'Destination Port', 'Protocol', 'Flow Duration',\n",
    "                'Total Fwd Packets', 'Total Backward Packets', 'Total Length of Fwd Packets',\n",
    "                'Total Length of Bwd Packets', 'Fwd Packet Length Max', 'Fwd Packet Length Min',\n",
    "                'Fwd Packet Length Mean', 'Fwd Packet Length Std', 'Bwd Packet Length Max',\n",
    "                'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std',\n",
    "                'Flow IAT Min', 'Flow IAT Max', 'Flow IAT Mean', 'Flow IAT Std',\n",
    "                'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Flow Packets/s',\n",
    "                'Flow Bytes/s'\n",
    "            ]\n",
    "            \n",
    "            # Verificar que las columnas existen\n",
    "            available_fields = [field for field in selected_fields if field in pcap_data.columns]\n",
    "            if not available_fields:\n",
    "                logger.warning(\"No se encontraron las columnas esperadas, usando todas las disponibles\")\n",
    "                selected_data = pcap_data\n",
    "            else:\n",
    "                logger.info(\"Los atributos de las trazas de red son correctos\")\n",
    "                selected_data = pcap_data[available_fields]\n",
    "            \n",
    "            # Mapeo de nombres de columnas\n",
    "            column_name_mapping = {\n",
    "                'src_ip': 'Source IP',\n",
    "                'dst_ip': 'Destination IP',\n",
    "                'src_port': 'Source Port',\n",
    "                'dst_port': 'Destination Port',\n",
    "                'protocol': 'Protocol',\n",
    "                'timestamp': 'Timestamp',\n",
    "                'flow_duration': 'Flow Duration',\n",
    "                'tot_fwd_pkts': 'Total Fwd Packets',\n",
    "                'tot_bwd_pkts': 'Total Backward Packets'\n",
    "            }\n",
    "            \n",
    "            # Renombrar columnas disponibles\n",
    "            rename_mapping = {k: v for k, v in column_name_mapping.items() if k in selected_data.columns}\n",
    "            selected_data = selected_data.rename(columns=rename_mapping)\n",
    "            \n",
    "            # Guardar dataset reducido\n",
    "            timestamp = os.path.splitext(os.path.basename(csv_path))[0]\n",
    "            #reduced_path = os.path.join(self.output_dir_prod, f\"{timestamp}_reduced.csv\")\n",
    "            \n",
    "            # Usamos Path para garantizar compatibilidad multiplataforma\n",
    "            reduced_path = Path(self.output_dir_prod) / f\"{timestamp}_reduced.csv\"\n",
    "            # Convertimos a string y normalizamos los separadores\n",
    "            reduced_path = os.path.normpath(reduced_path)\n",
    "            \n",
    "            selected_data.to_csv(reduced_path, index=True)\n",
    "            \n",
    "            logger.info(f\"Dataset reducido guardado: {reduced_path}\")\n",
    "            return reduced_path\n",
    "            \n",
    "        except Exception as ex:\n",
    "            logger.error(f\"Error reduciendo dataset: {ex}\")\n",
    "            raise\n",
    "\n",
    "class MLClassifier:\n",
    "    \"\"\"Módulo de clasificación usando modelos ML\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str):\n",
    "        self.model_path = model_path\n",
    "        self.model = None\n",
    "        self.feature_columns = None\n",
    "        self.load_model()\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Carga el modelo ML desde archivo\"\"\"\n",
    "        try:\n",
    "            if self.model_path.endswith('.joblib'):\n",
    "                self.model = joblib.load(self.model_path)\n",
    "            elif self.model_path.endswith('.pkl') or self.model_path.endswith('.pickle'):\n",
    "                with open(self.model_path, 'rb') as f:\n",
    "                    self.model = pickle.load(f)\n",
    "            else:\n",
    "                raise ValueError(\"Formato de modelo no soportado. Use .joblib, .pkl o .pickle\")\n",
    "            \n",
    "            logger.info(f\"Modelo cargado exitosamente: {self.model_path}\")\n",
    "            \n",
    "        except Exception as ex:\n",
    "            logger.error(f\"Error cargando modelo: {ex}\")\n",
    "            raise\n",
    "    \n",
    "    def prepare_features(self, data: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Prepara características asegurando compatibilidad con los feature names del modelo\"\"\"\n",
    "        try:\n",
    "            # 1. Limpieza inicial silenciosa\n",
    "            data = data.drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "        \n",
    "            # 2. Identificar/validar columnas necesarias\n",
    "            required_features = [\n",
    "                'Source Port', 'Destination Port', 'Protocol', 'Flow Duration',\n",
    "                'Total Fwd Packets', 'Total Backward Packets', 'Total Length of Fwd Packets',\n",
    "                'Total Length of Bwd Packets', 'Fwd Packet Length Max', 'Fwd Packet Length Min',\n",
    "                'Fwd Packet Length Mean', 'Fwd Packet Length Std', 'Bwd Packet Length Max',\n",
    "                'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std',\n",
    "                'Flow IAT Min', 'Flow IAT Max', 'Flow IAT Mean', 'Flow IAT Std',\n",
    "                'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Flow Packets/s',\n",
    "                'Flow Bytes/s'\n",
    "            ]\n",
    "        \n",
    "            # 3. Verificación estricta de columnas\n",
    "            missing = set(required_features) - set(data.columns)\n",
    "            if missing:\n",
    "                raise ValueError(f\"Features faltantes: {missing}\")\n",
    "        \n",
    "            # 4. Orden exacto y conversión\n",
    "            features = data[required_features].fillna(0)\n",
    "        \n",
    "            # 5. Conversión que preserva feature names\n",
    "            from sklearn.utils.validation import check_array\n",
    "            return check_array(\n",
    "                features, \n",
    "                accept_sparse=False,  # Más eficiente para datos densos\n",
    "                ensure_all_finite=True,\n",
    "                dtype=np.float64\n",
    "            )\n",
    "        \n",
    "        except Exception as ex:\n",
    "            logger.error(f\"Error preparando features: {str(ex)}\")\n",
    "            raise\n",
    "    \n",
    "    def classify(self, data: pd.DataFrame) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Clasifica los datos y retorna predicciones\"\"\"\n",
    "        try:\n",
    "            if self.model is None:\n",
    "                raise ValueError(\"Modelo no cargado\")\n",
    "            \n",
    "            features = self.prepare_features(data)\n",
    "            ## Correcciones de los Warnings\n",
    "            \n",
    "            predictions = self.model.predict(features)\n",
    "            \n",
    "            # Si el modelo soporta probabilidades\n",
    "            try:\n",
    "                probabilities = self.model.predict_proba(features)\n",
    "                max_probs = np.max(probabilities, axis=1)\n",
    "            except:\n",
    "                max_probs = np.ones(len(predictions))  # Confianza por defecto\n",
    "            \n",
    "            results = []\n",
    "            for i, (pred, prob) in enumerate(zip(predictions, max_probs)):\n",
    "                results.append({\n",
    "                    'row_index': i,\n",
    "                    'prediction': pred,\n",
    "                    'confidence': float(prob),\n",
    "                    'is_attack': pred != 0 if isinstance(pred, (int, np.integer)) else 'attack' in str(pred).lower()\n",
    "                })\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as ex:\n",
    "            logger.error(f\"Error en clasificación: {ex}\")\n",
    "            raise\n",
    "\n",
    "class BDINetworkAgent:\n",
    "    \"\"\"Agente BDI principal para detección de ataques de red\"\"\"\n",
    "    \n",
    "    def __init__(self, interface: str = \"wlp1s0\", model_path: str = \"model.joblib\"):\n",
    "        self.interface = interface\n",
    "        self.model_path = model_path\n",
    "        \n",
    "        # Componentes del agente\n",
    "        self.network_capture = NetworkCapture(interface)\n",
    "        self.ml_classifier = MLClassifier(model_path)\n",
    "        \n",
    "        # Estado BDI\n",
    "        self.beliefs: Dict[str, Belief] = {}\n",
    "        self.desires: List[Desire] = []\n",
    "        self.intentions: queue.Queue = queue.Queue()\n",
    "        self.current_state = AgentState.IDLE\n",
    "        \n",
    "        # Configuración\n",
    "        self.capture_duration = 30\n",
    "        self.monitoring_interval = 60\n",
    "        self.threat_threshold = 0.7\n",
    "        \n",
    "        # Cola de alertas\n",
    "        self.alerts: queue.Queue = queue.Queue()\n",
    "        \n",
    "        # Control de threads\n",
    "        self.running = False\n",
    "        self.monitoring_thread = None\n",
    "        \n",
    "        self._initialize_desires()\n",
    "    \n",
    "    def _initialize_desires(self):\n",
    "        \"\"\"Inicializa los deseos/objetivos del agente\"\"\"\n",
    "        self.desires = [\n",
    "            Desire(\"monitor_network\", priority=1),\n",
    "            Desire(\"detect_threats\", priority=2),\n",
    "            Desire(\"respond_to_attacks\", priority=3),\n",
    "            Desire(\"maintain_security\", priority=4)\n",
    "        ]\n",
    "    \n",
    "    def update_belief(self, key: str, value: Any, confidence: float = 1.0):\n",
    "        \"\"\"Actualiza una creencia del agente\"\"\"\n",
    "        self.beliefs[key] = Belief(key, value, confidence)\n",
    "        logger.debug(f\"Creencia actualizada: {key} = {value} (confianza: {confidence})\")\n",
    "    \n",
    "    def add_intention(self, action: str, parameters: Dict[str, Any], expected_outcome: str):\n",
    "        \"\"\"Añade una nueva intención a la cola\"\"\"\n",
    "        intention = Intention(action, parameters, expected_outcome)\n",
    "        self.intentions.put(intention)\n",
    "        logger.debug(f\"Intención añadida: {action}\")\n",
    "    \n",
    "    def process_network_data(self, csv_path: str) -> List[ThreatAlert]:\n",
    "        \"\"\"Procesa datos de red y genera alertas\"\"\"\n",
    "        try:\n",
    "            self.current_state = AgentState.ANALYZING\n",
    "            \n",
    "            # Cargar datos\n",
    "            data = pd.read_csv(csv_path)\n",
    "            _description = data.describe()\n",
    "            logger.info(f\"(process_network_data) Procesando {len(data)} registros de red\")\n",
    "            \n",
    "            \n",
    "            # Clasificar con ML\n",
    "            predictions = self.ml_classifier.classify(data)\n",
    "            \n",
    "            # Generar alertas\n",
    "            alerts = []\n",
    "            attack_count = 0\n",
    "            \n",
    "            for pred in predictions:\n",
    "                if pred['is_attack'] and pred['confidence'] >= self.threat_threshold:\n",
    "                    attack_count += 1\n",
    "                    \n",
    "                    # Extraer información del registro\n",
    "                    row = data.iloc[pred['row_index']]\n",
    "                    \n",
    "                    # Determinar nivel de amenaza\n",
    "                    if pred['confidence'] >= 0.9:\n",
    "                        threat_level = ThreatLevel.CRITICAL\n",
    "                    elif pred['confidence'] >= 0.8:\n",
    "                        threat_level = ThreatLevel.HIGH\n",
    "                    elif pred['confidence'] >= 0.7:\n",
    "                        threat_level = ThreatLevel.MEDIUM\n",
    "                    else:\n",
    "                        threat_level = ThreatLevel.LOW\n",
    "                    \n",
    "                    alert = ThreatAlert(\n",
    "                        timestamp=datetime.now(),\n",
    "                        threat_level=threat_level,\n",
    "                        attack_type=str(pred['prediction']),\n",
    "                        confidence=pred['confidence'],\n",
    "                        source_ip=row.get('Source IP', 'unknown'),\n",
    "                        destination_ip=row.get('Destination IP', 'unknown'),\n",
    "                        details={\n",
    "                            'row_index': pred['row_index'],\n",
    "                            'prediction': pred['prediction'],\n",
    "                            'raw_data': row.to_dict()\n",
    "                        }\n",
    "                    )\n",
    "                    \n",
    "                    alerts.append(alert)\n",
    "                    self.alerts.put(alert)\n",
    "            \n",
    "            # Actualizar creencias\n",
    "            self.update_belief(\"last_analysis_time\", datetime.now())\n",
    "            self.update_belief(\"total_flows_analyzed\", len(data))\n",
    "            self.update_belief(\"attacks_detected\", attack_count)\n",
    "            self.update_belief(\"attack_rate\", attack_count / len(data) if len(data) > 0 else 0)\n",
    "            \n",
    "            logger.info(f\"Análisis completado: {attack_count} ataques detectados de {len(data)} flujos\")\n",
    "            \n",
    "            return alerts\n",
    "            \n",
    "        except Exception as ex:\n",
    "            logger.error(f\"Error procesando datos de red: {ex}\")\n",
    "            return []\n",
    "    \n",
    "    # Antes de procesar, verifica y ajusta las columnas\n",
    "    \n",
    "    def capture_and_analyze(self):\n",
    "        \"\"\"Captura tráfico y lo analiza\"\"\"\n",
    "        try:\n",
    "            \n",
    "            self.current_state = AgentState.CAPTURING\n",
    "            logger.info(\"Iniciando captura de tráfico...\")\n",
    "            \n",
    "            # Capturar tráfico\n",
    "            _pcap_path = self.network_capture.capture_traffic(self.capture_duration)\n",
    "            _output_path = './output.csv'\n",
    "            self.current_state = AgentState.PROCESSING\n",
    "            logger.info(\"Procesando captura...\")\n",
    "            \n",
    "            csv_path = Path(self.network_capture.extract_flow_stats_with_tshark(pcap_path=_pcap_path, output_csv=_output_path))\n",
    "\n",
    "            reduced_csv_path = self.network_capture.reduce_dataset(csv_path)\n",
    "            \n",
    "            # Analizar con ML\n",
    "            alerts = self.process_network_data(reduced_csv_path)\n",
    "            \n",
    "            # Responder a amenazas críticas\n",
    "            critical_alerts = [a for a in alerts if a.threat_level == ThreatLevel.CRITICAL]\n",
    "            if critical_alerts:\n",
    "                self.add_intention(\"respond_to_critical_threat\", \n",
    "                                 {\"alerts\": critical_alerts}, \n",
    "                                 \"threat_mitigated\")\n",
    "            \n",
    "            self.current_state = AgentState.IDLE\n",
    "            \n",
    "        except Exception as ex:\n",
    "            logger.error(f\"Error en captura y análisis: {ex}\")\n",
    "            self.current_state = AgentState.IDLE\n",
    "    \n",
    "    def execute_intentions(self):\n",
    "        \"\"\"Ejecuta las intenciones pendientes\"\"\"\n",
    "        while self.running:\n",
    "            try:\n",
    "                intention = self.intentions.get(timeout=1)\n",
    "                \n",
    "                if intention.action == \"respond_to_critical_threat\":\n",
    "                    self._respond_to_threats(intention.parameters[\"alerts\"])\n",
    "                    intention.status = \"completed\"\n",
    "                \n",
    "                self.intentions.task_done()\n",
    "                \n",
    "            except queue.Empty:\n",
    "                continue\n",
    "            except Exception as ex:\n",
    "                logger.error(f\"Error ejecutando intención: {ex}\")\n",
    "    \n",
    "    def _respond_to_threats(self, alerts: List[ThreatAlert]):\n",
    "        \"\"\"Responde a amenazas críticas\"\"\"\n",
    "        self.current_state = AgentState.RESPONDING\n",
    "        \n",
    "        for alert in alerts:\n",
    "            logger.warning(f\"AMENAZA CRÍTICA DETECTADA: {alert.attack_type} desde {alert.source_ip}\")\n",
    "            logger.warning(f\"Confianza: {alert.confidence:.2%}\")\n",
    "            logger.warning(f\"Detalles: {alert.details}\")\n",
    "            \n",
    "            # Aquí podrías implementar respuestas automáticas como:\n",
    "            # - Bloquear IP en firewall\n",
    "            # - Enviar notificaciones\n",
    "            # - Registrar en SIEM\n",
    "            # - Etc.\n",
    "        \n",
    "        self.current_state = AgentState.IDLE\n",
    "    \n",
    "    def monitoring_loop(self):\n",
    "        \"\"\"Bucle principal de monitoreo\"\"\"\n",
    "        while self.running:\n",
    "            try:\n",
    "                logger.info(\"Iniciando ciclo de monitoreo...\")\n",
    "                self.capture_and_analyze()\n",
    "                \n",
    "                # Esperar antes del siguiente ciclo\n",
    "                time.sleep(self.monitoring_interval)\n",
    "                \n",
    "            except Exception as ex:\n",
    "                logger.error(f\"Error en bucle de monitoreo: {ex}\")\n",
    "                time.sleep(10)  # Pausa antes de reintentar\n",
    "    \n",
    "    def start_monitoring(self):\n",
    "        \"\"\"Inicia el monitoreo continuo\"\"\"\n",
    "        if self.running:\n",
    "            logger.warning(\"El monitoreo ya está en ejecución\")\n",
    "            return\n",
    "        \n",
    "        self.running = True\n",
    "        logger.info(\"Iniciando agente BDI de seguridad de red...\")\n",
    "        \n",
    "        # Iniciar thread de monitoreo\n",
    "        self.monitoring_thread = threading.Thread(target=self.monitoring_loop)\n",
    "        self.monitoring_thread.daemon = True\n",
    "        self.monitoring_thread.start()\n",
    "        \n",
    "        # Iniciar thread de ejecución de intenciones\n",
    "        intentions_thread = threading.Thread(target=self.execute_intentions)\n",
    "        intentions_thread.daemon = True\n",
    "        intentions_thread.start()\n",
    "        \n",
    "        logger.info(\"Agente BDI iniciado correctamente\")\n",
    "    \n",
    "    def stop_monitoring(self):\n",
    "        \"\"\"Detiene el monitoreo\"\"\"\n",
    "        if not self.running:\n",
    "            logger.warning(\"El monitoreo no está en ejecución\")\n",
    "            return\n",
    "        \n",
    "        logger.info(\"Deteniendo agente BDI...\")\n",
    "        self.running = False\n",
    "        \n",
    "        if self.monitoring_thread:\n",
    "            self.monitoring_thread.join(timeout=5)\n",
    "        \n",
    "        logger.info(\"Agente BDI detenido\")\n",
    "    \n",
    "    def get_status(self) -> Dict[str, Any]:\n",
    "        \"\"\"Obtiene el estado actual del agente\"\"\"\n",
    "        return {\n",
    "            \"state\": self.current_state.value,\n",
    "            \"beliefs\": {k: {\"value\": v.value, \"confidence\": v.confidence, \"timestamp\": v.timestamp} \n",
    "                       for k, v in self.beliefs.items()},\n",
    "            \"active_desires\": [d.goal for d in self.desires if d.active],\n",
    "            \"pending_intentions\": self.intentions.qsize(),\n",
    "            \"pending_alerts\": self.alerts.qsize()\n",
    "        }\n",
    "    \n",
    "    def get_recent_alerts(self, limit: int = 10) -> List[ThreatAlert]:\n",
    "        \"\"\"Obtiene las alertas recientes\"\"\"\n",
    "        alerts = []\n",
    "        temp_alerts = []\n",
    "        \n",
    "        # Extraer alertas de la cola\n",
    "        while not self.alerts.empty() and len(alerts) < limit:\n",
    "            alert = self.alerts.get()\n",
    "            alerts.append(alert)\n",
    "            temp_alerts.append(alert)\n",
    "        \n",
    "        # Devolver alertas a la cola\n",
    "        for alert in temp_alerts:\n",
    "            self.alerts.put(alert)\n",
    "        \n",
    "        return alerts\n",
    "\n",
    "def main():\n",
    "    \"\"\"Función principal para demostrar el uso del agente\"\"\"\n",
    "    \n",
    "    # Configuración\n",
    "    INTERFACE = \"eth0\"  # Interfaz de red\n",
    "    MODEL_PATH = \"./models/ada_boost_training_exp_attack_tcp.joblib\"  # Ruta a tu modelo ML\n",
    "    \n",
    "    # Verificar que existe el modelo\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        logger.error(f\"Modelo no encontrado: {MODEL_PATH}\")\n",
    "        logger.info(\"Por favor, proporciona un modelo válido (.joblib, .pkl, .pickle)\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Crear y configurar el agente\n",
    "        agent = BDINetworkAgent(interface=INTERFACE, model_path=MODEL_PATH)\n",
    "        \n",
    "        # Configurar parámetros\n",
    "        agent.capture_duration = 30  # 30 segundos de captura\n",
    "        agent.monitoring_interval = 120  # Analizar cada 2 minutos\n",
    "        agent.threat_threshold = 0.6  # Umbral de confianza para alertas\n",
    "        \n",
    "        # Iniciar monitoreo\n",
    "        agent.start_monitoring()\n",
    "        \n",
    "        # Monitoreo interactivo\n",
    "        print(\"\\n=== AGENTE BDI DE SEGURIDAD DE RED ===\")\n",
    "        print(\"Comandos disponibles:\")\n",
    "        print(\"  status  - Ver estado del agente\")\n",
    "        print(\"  alerts  - Ver alertas recientes\")\n",
    "        print(\"  capture - Forzar captura manual\")\n",
    "        print(\"  quit    - Salir\")\n",
    "        print(\"=====================================\\n\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                command = input(\">>> \").strip().lower()\n",
    "                \n",
    "                if command == \"quit\":\n",
    "                    break\n",
    "                elif command == \"status\":\n",
    "                    status = agent.get_status()\n",
    "                    print(f\"\\nEstado: {status['state']}\")\n",
    "                    print(f\"Creencias activas: {len(status['beliefs'])}\")\n",
    "                    print(f\"Intenciones pendientes: {status['pending_intentions']}\")\n",
    "                    print(f\"Alertas pendientes: {status['pending_alerts']}\")\n",
    "                    for belief, data in status['beliefs'].items():\n",
    "                        print(f\"  {belief}: {data['value']} (confianza: {data['confidence']:.2f})\")\n",
    "                \n",
    "                elif command == \"alerts\":\n",
    "                    alerts = agent.get_recent_alerts(5)\n",
    "                    if alerts:\n",
    "                        print(f\"\\n=== ALERTAS RECIENTES ({len(alerts)}) ===\")\n",
    "                        for alert in alerts:\n",
    "                            print(f\"[{alert.timestamp.strftime('%H:%M:%S')}] \"\n",
    "                                  f\"{alert.threat_level.value.upper()}: {alert.attack_type}\")\n",
    "                            print(f\"  Desde: {alert.source_ip} -> {alert.destination_ip}\")\n",
    "                            print(f\"  Confianza: {alert.confidence:.2%}\")\n",
    "                    else:\n",
    "                        print(\"No hay alertas recientes\")\n",
    "                \n",
    "                elif command == \"capture\":\n",
    "                    print(\"Forzando captura manual...\")\n",
    "                    threading.Thread(target=agent.capture_and_analyze).start()\n",
    "                \n",
    "                else:\n",
    "                    print(\"Comando no reconocido\")\n",
    "                    \n",
    "            except KeyboardInterrupt:\n",
    "                break\n",
    "            except Exception as ex:\n",
    "                logger.error(f\"Error en comando: {ex}\")\n",
    "        \n",
    "        # Detener el agente\n",
    "        agent.stop_monitoring()\n",
    "        print(\"Agente detenido.\")\n",
    "        \n",
    "    except Exception as ex:\n",
    "        logger.error(f\"Error iniciando agente: {ex}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583b9610-b455-4876-b128-cc008aa5100d",
   "metadata": {},
   "source": [
    "## Integración de Agentes BDI en Contenedores Docker\n",
    "La contenerización con Docker ofrece una solución robusta y reproducible para desplegar agentes BDI (Beliefs, Desires, Intentions). Al encapsular el agente y todas sus dependencias en un entorno aislado, garantizamos que el agente funcione de manera consistente en cualquier sistema.\n",
    "\n",
    "1. Requisitos y Dependencias\n",
    "Para construir el contenedor, primero necesitas definir todas las dependencias necesarias. Esto se hace a través de un archivo de requisitos, lo que facilita la instalación automática durante la construcción de la imagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26794bd6-f932-4f89-9f70-387663bf7f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements.txt:\n",
    "\n",
    "# Ejemplo de dependencias para un agente BDI\n",
    "# Sustituye por las librerías que uses (ej. para un agente en Python)\n",
    "numpy==1.22.3\n",
    "pandas==1.4.2\n",
    "scikit-learn==1.0.2\n",
    "matplotlib==3.5.2\n",
    "tensorflow==2.8.0\n",
    "scipy==1.8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fa30aa-03b6-48ff-9a92-bc3683a860ee",
   "metadata": {},
   "source": [
    "2. Construcción de la Imagen Docker\n",
    "La construcción de la imagen se realiza mediante un archivo Dockerfile. Este archivo contiene las instrucciones para crear un entorno de ejecución, instalar las dependencias y copiar el código de tu agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f15c2d4-3494-4e2c-8c21-9c8f8e78a686",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dockerfile:\n",
    "\n",
    "\n",
    "# Usa una imagen base de Python oficial, que ya incluye el entorno de ejecución necesario\n",
    "FROM python:3.9-slim\n",
    "\n",
    "# Establece el directorio de trabajo dentro del contenedor\n",
    "WORKDIR /app\n",
    "\n",
    "# Copia los archivos de requisitos e instala las dependencias\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copia el código de tu aplicación de agente al directorio de trabajo\n",
    "# 'bdi_agent/' es la carpeta donde está tu código fuente\n",
    "COPY bdi_agent/ .\n",
    "\n",
    "# Expone el puerto si tu agente necesita comunicarse (opcional)\n",
    "# Por ejemplo, si el agente tiene una interfaz web o API REST\n",
    "# EXPOSE 8080\n",
    "\n",
    "# Comando para ejecutar el agente cuando el contenedor se inicie\n",
    "# Asegúrate de que 'main.py' es el archivo de entrada de tu agente\n",
    "CMD [\"python\", \"main.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce03a4d3-1629-47a8-9d45-91542ece810e",
   "metadata": {},
   "source": [
    "## Construcción y Ejecución del Contenedor\n",
    "Una vez que tengas tu requirements.txt y tu Dockerfile en el mismo directorio que el código de tu agente (bdi_agent/), puedes construir la imagen y ejecutar el contenedor con los siguientes comandos:\n",
    "\n",
    "### Construir la Imagen \n",
    "Ejecuta el siguiente comando en tu terminal. El punto (.) al final indica que Docker debe buscar el Dockerfile en el directorio actual.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b967e4-9623-409c-bb7c-510801668a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! sudo docker build -t asddosdetect ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c47eff-2088-45f1-b5c1-80ad36718757",
   "metadata": {},
   "source": [
    "### docker build: Comando para construir una imagen.\n",
    "\n",
    "\n",
    "-t  mi-agente-bdi: Asigna un nombre (etiqueta o \"tag\") a la imagen. </br>\n",
    ".   El contexto de construcción, que es el directorio actual.</br>\n",
    "\n",
    "### Ejecutar el Contenedor \n",
    "Una vez que la imagen ha sido construida exitosamente, puedes ejecutar el agente dentro de un contenedor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abc55f3a-ed67-42c6-b7d0-2a8fe25c4ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! sudo docker run --rm -it --privileged asddosdetect bash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f8518d-17c0-42d6-8612-ee4f57082455",
   "metadata": {},
   "source": [
    "### docker run: Comando para crear y ejecutar un contenedor a partir de una imagen.\n",
    "\n",
    "-d: Ejecuta el contenedor en modo detached (segundo plano). </br>\n",
    "--name agente-en-ejecucion: Asigna un nombre al contenedor para poder gestionarlo fácilmente. </br>\n",
    "mi-agente-bdi: El nombre de la imagen que creaste. </br>\n",
    "\n",
    "Comandos Adicionales Útiles:\n",
    "- Para ver los contenedores en ejecución: docker ps\n",
    "- Para ver los registros (logs) del agente: docker logs agente-en-ejecucion\n",
    "- Para detener el contenedor: docker stop agente-en-ejecucion\n",
    "- Para eliminar el contenedor (una vez detenido): docker rm agente-en-ejecucion\n",
    "\n",
    "Esta metodología garantiza que tu agente BDI se pueda desplegar de forma predecible y consistente, sin importar el entorno de ejecución subyacente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154d6d35-6f86-40ef-b86c-7ae5c7e57a9e",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53232563-3679-4ab7-8b65-30187d4940d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "2025-08-07 12:28:25,081 - __main__ - INFO - Modelo cargado exitosamente: ./models/ada_boost_training_exp_attack_tcp.joblib\n",
    "2025-08-07 12:28:25,081 - __main__ - INFO - Iniciando agente BDI de seguridad de red...\n",
    "2025-08-07 12:28:25,083 - __main__ - INFO - Iniciando ciclo de monitoreo...\n",
    "2025-08-07 12:28:25,083 - __main__ - INFO - Iniciando captura de tráfico...\n",
    "2025-08-07 12:28:25,084 - __main__ - INFO - Ejecutando captura: tshark -i eth0 -a duration:30 -w pcap\\07082025-122825.pcap >> standard.out \n",
    "2025-08-07 12:28:25,086 - __main__ - INFO - Agente BDI iniciado correctamente\n",
    "\n",
    "=== AGENTE BDI DE SEGURIDAD DE RED ===\n",
    "Comandos disponibles:\n",
    "  status  - Ver estado del agente\n",
    "  alerts  - Ver alertas recientes\n",
    "  capture - Forzar captura manual\n",
    "  quit    - Salir\n",
    "=====================================\n",
    "\n",
    ">>> Capturing on 'Wi-Fi'\n",
    "6112 \n",
    "2025-08-07 12:29:01,353 - __main__ - INFO - Procesando captura...\n",
    "2025-08-07 12:29:02,403 - __main__ - INFO - Dataset cargado: output.csv, shape: (93, 25)\n",
    "2025-08-07 12:29:02,404 - __main__ - INFO - Los atributos de las trazas de red son correctos\n",
    "2025-08-07 12:29:02,415 - __main__ - INFO - Dataset reducido guardado: prod\\output_reduced.csv\n",
    "2025-08-07 12:29:02,463 - __main__ - INFO - (process_network_data) Procesando 93 registros de red\n",
    "2025-08-07 12:29:02,498 - __main__ - INFO - Análisis completado: 1 ataques detectados de 93 flujos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
